---
title: "Eliminating Racial Bias Relating to Violent Crimes in U.S. Communities using a Predictive Machine Learning Model"
author: "input our names here"
format: pdf
editor: visual
---

# Reading Libraries and Dataset

```{r}
#loads packages
packages <- c(
    # Old packages
   "tidyverse",
    "ISLR2",
    "dplyr",
    "tidyr",
    "readr",
    "purrr",
    "glmnet",
    "caret",
    "repr",
    # NEW
    "torch",
    "mlbench",
    "Stat2Data",
    "ggformula",
    "mosaic",
    "car"
)

CrimeData <- read.csv("https://raw.githubusercontent.com/snehaprem03/StatProject/main/communities.data", header = TRUE, na.strings = "?")

#This reads the csv file, addresses the first row as a header, and then changes all the "?" values to NA in order to make it easier to change later
```

\-

# Introduction to our Data

## **Research Question:** 

\"Evaluating Racial Bias Relating to Violent Crimes in U.S. Communities using a Predictive Machine Learning Model.\"

## **Why is this problem important?**

This problem is important because we want to build a predictive model for the number of violent crimes within U.S. communities. There are many different policy decisions made based on various attributes like race, gender, police force per area, income per area in a community and with the number of violent crimes in the U.S. increasing we wanted to see which of these attributes contribute the most to a high-crime U.S community.Â 

Most models often conclude that black communities have higher violent crime rates, however we want to eliminate this racial bias in our model and see if it is actually race that affects violent crimes or if it is more nuanced features such as a large police force, high unemployment, etc. This can be very important for policy decisions

# 1. Data Tidying and Manipulation

### 1.1 Understanding Our Data

```{r}
head(CrimeData) #first few rows
str(CrimeData) # structure of the data set
```

```{r}
summary(CrimeData) #summary statistics + distribution of each variable
```

```{r}
CrimeData %>% summarise_all(n_distinct) #number of unique values for each column of the data frame
```

### 1.2 Cleaning and Pre-Processing Data

```{r}
CrimeData %>% summarise_all(~ sum(is.na(.)))
# We can see that country, community, LemasSwornFT, LemasSwFTPerPop, LemasSwFTFieldOps, LemasSwFTFieldPerPop, LemasTotalReq, LemasTotReqPerPop, PolicReqPerOffic, PolicPerPo, RacialMatchCom, PctPolicWhite, PctPolicBlack, PctPolicHisp, PctPolicAsian, PctPolicMinor, OfficAssgnDrugUnits, NumKindsDrugsSeiz, PolicAveOTWorked, PolicCars, PolicOperBudg, LemasPctPolicOnPatr, LemasGangUnitDeploy, PolicBudgPerPop all have missing values (1675 for almost all but 1174 for the first two)

```

```{r}
# The LEMAS data seems to have a lot of missing values. About 1675/1994 or 84% of the LEMAS columns have missing data. So, the best way to handle this is probably to drop those columns


# Identify columns with more than 80% missing data
na_percent <- apply(is.na(CrimeData), 2, mean)
cols_to_drop <- names(na_percent[na_percent > 0.8])

# List Dropped Columns
cat("Dropped columns:", paste(cols_to_drop, collapse = ", "))

# Drop the columns with more than 80% missing data
CrimeData <- CrimeData[, !names(CrimeData) %in% cols_to_drop]



```

```{r}
# Dropping the County and Community columns"

CrimeData <- CrimeData[, !(names(CrimeData) %in% c("country", "community"))]

```

```{r}
# OtherPerCap also has one missing row. We can use the mean of the column to replace this NA value since it is the only one

mean_other_percap <- mean(CrimeData$OtherPerCap, na.rm = TRUE)
CrimeData$OtherPerCap[is.na(CrimeData$OtherPerCap)] <- mean_other_percap
```

```{r}
# Final Check for Any Missing Values
CrimeData %>% summarise_all(~ sum(is.na(.)))

```

# Variable Creation, Selection, and Manipulation

```{r}

```

\-

# Introductory Plots

```{r}
#This creates a quick graph comparing violent crimes and median income
#gf_point(ViolentCrimesPerPop ~ medIncome, data = CrimeData)
```

\-

# Exploratory Data Analysis

```{r}

```

Here we are testing for multicollinearity in our data. Any data that has a value greater than 5 creates the risk of multicollinearity. To avoid this, we will need to remove columns that are very similar in their predictive strength.

```{r}

#We will likely use the following code, once we make our model, to test for multicollinearity.
#vif(model) %>% knitr::kable()
```

\-

## What are your expected outcomes for the project? 

Initially, we would want to be able to find the predictors from our dataset that impact the likelihood of violent crimes occurring, which will allow us to have a better understanding of the data, which will give us a stronger model when making predictions regarding violent crimes occurring. Other factors that may influence this can be variables such as race, gender, police force per area, income, etc. From all this, we would have a classification model built on feature selection and encoding algorithms (the number of violent crimes that occur).
